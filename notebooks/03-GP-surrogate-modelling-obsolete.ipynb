{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a surrogate model of EPANET's water quality solver using Gaussian Process (GP) regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "default_colors = plotly.colors.qualitative.Plotly\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "from pyDOE import lhs\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from bayesian_wq_calibration.epanet import build_model, sensor_model_id, epanet_simulator, set_reaction_parameters\n",
    "from bayesian_wq_calibration.mcmc import decision_variables_to_dict\n",
    "from bayesian_wq_calibration.constants import TIMESERIES_DIR, RESULTS_DIR\n",
    "from bayesian_wq_calibration.data import bulk_temp_adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load operational data for selected sensing period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_period = 18 # 21 calibration events (as at 31 October 2024)\n",
    "try:\n",
    "    flow_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-flow.csv\")\n",
    "    pressure_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-pressure.csv\")\n",
    "    wq_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-wq.csv\", low_memory=False)\n",
    "    cl_df = wq_df[wq_df['data_type'] == 'chlorine']\n",
    "except:\n",
    "    print(f\"Data period {data_period} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate model data period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_days = 2\n",
    "\n",
    "n_total = len(flow_df['datetime'].unique())\n",
    "n_surrogate = surrogate_days * 24 * 4\n",
    "surrogate_range = range(n_surrogate)\n",
    "surrogate_datetime = flow_df['datetime'].unique()[list(surrogate_range)]\n",
    "total_range = range(n_total)\n",
    "total_datetime = flow_df['datetime'].unique()[list(total_range)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_coeff = -0.85 # day^-1 (from bottle tests)\n",
    "field_temp = wq_df[wq_df['data_type'] == 'temperature']['mean'].mean()\n",
    "bulk_coeff = bulk_temp_adjust(bulk_coeff, field_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall decay grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = 'material-velocity' # 'single', 'material', 'material-diameter', 'material-velocity'\n",
    "\n",
    "if grouping == 'single':\n",
    "    param_bounds = [(-1.0, 0.0)] # single wall decay coefficient\n",
    "elif grouping == 'material':\n",
    "    param_bounds = [(-1.0, 0.0), (-0.25, 0.0), (-0.1, 0.0)] # variable order: metallic, cement, plastic_unknown\n",
    "elif grouping == 'material-diameter':\n",
    "    param_bounds = [(-1.0, 0.0), (-0.5, 0.0), (-0.25, 0.0), (-0.1, 0.0)] # variable order: metallic_less_than_150, metallic_greater_than_150, cement, plastic_unknown\n",
    "elif grouping == 'material-velocity':\n",
    "    param_bounds = [(-1.0, 0.0), (-0.5, 0.0), (-0.5, 0.0), (-0.25, 0.0), (-0.25, 0.0), (-0.1, 0.0)] # variable order: metallic_low_velocity, metallic_high_velocity, cement_low_velocity, cement_high_velocity, plastic_low_velocity, plastic_high_velocity\n",
    "\n",
    "wall_params = [random.uniform(lower, upper) for lower, upper in param_bounds]\n",
    "n_params = len(wall_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EPANET simulator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build water model using `wntr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_resolution = 'wwmd'\n",
    "wn = build_model(flow_df[flow_df['datetime'].isin(surrogate_datetime)], pressure_df[pressure_df['datetime'].isin(surrogate_datetime)], cl_df[cl_df['datetime'].isin(surrogate_datetime)], sim_type='chlorine', demand_resolution=demand_resolution, bulk_coeff=bulk_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean velocities (for 'material-velocity' grouping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grouping == 'material-velocity':\n",
    "    sim_results = epanet_simulator(wn, 'velocity', cl_df[cl_df['datetime'].isin(surrogate_datetime)])\n",
    "    vel_sim = sim_results.velocity.T\n",
    "    mean_vel = vel_sim.mean(axis=1)\n",
    "    mean_vel = mean_vel.reset_index().rename(columns={'name': 'link_id', 0: 'mean_vel'})\n",
    "else:\n",
    "    mean_vel = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define simualtor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(cl_df, wall_params, wn, grouping, mean_vel):\n",
    "    wall_params = decision_variables_to_dict(grouping, wall_params)\n",
    "    _wn = set_reaction_parameters(wn, grouping, wall_params, None, mean_vel)\n",
    "    \n",
    "    sim_type = 'chlorine'\n",
    "    sim_results = epanet_simulator(_wn, sim_type, cl_df)\n",
    "    cl_sim = sim_results.chlorine\n",
    "    \n",
    "    sensor_data = sensor_model_id('wq')\n",
    "    cl_sim = cl_sim[sensor_data['model_id'].unique()]\n",
    "    name_mapping = sensor_data.set_index('model_id')['bwfl_id'].to_dict()\n",
    "    cl_sim = cl_sim.rename(columns=name_mapping)\n",
    "\n",
    "    cl_sim = cl_sim.T\n",
    "    cl_sim.columns = [f't_{idx+1}' for idx in range(cl_sim.shape[1])]\n",
    "\n",
    "    cl_sim = cl_sim.drop(index=['BW1', 'BW4'], errors='ignore') # remove inlet sensors\n",
    "    \n",
    "    return cl_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_simulator = simulator(cl_df[cl_df['datetime'].isin(surrogate_datetime)], wall_params, wn, grouping, mean_vel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design of experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latin Hypercube Sampling (LHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_lhs(param_bounds, n_params, n_samples):\n",
    "    samples = lhs(n_params, samples=n_samples)\n",
    "    scaled_samples = np.array([\n",
    "        low + (high - low) * sample\n",
    "        for (low, high), sample in zip(param_bounds, samples.T)\n",
    "    ]).T\n",
    "    return scaled_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = [10, 25, 50, 100, 200]\n",
    "n_samples_idx = 2\n",
    "\n",
    "X = experiments_lhs(param_bounds, n_params, n_samples[n_samples_idx])\n",
    "Y = np.array([\n",
    "    simulator(cl_df[cl_df['datetime'].isin(surrogate_datetime)], params, wn, grouping, mean_vel)\n",
    "    for params in X\n",
    "])\n",
    "Y_flat = Y.reshape(n_samples[n_samples_idx], Y.shape[1] * Y.shape[2]) # reshape for GP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gausian process model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure using five-fold cross-validation and `GPyTorch` modules. The following kernel's can be used:\n",
    "- Radial basis function (RBF)\n",
    "- Matern\n",
    "- Rational quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel, MaternKernel, ScaleKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "from gpytorch.models import ExactGP\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_tasks, kernel_type='RBF', nu=1.5):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=num_tasks\n",
    "        )\n",
    "        \n",
    "        if kernel_type == \"RBF\":\n",
    "            self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "                gpytorch.kernels.RBFKernel(lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(1e-1, 1e10)), num_tasks=num_tasks, rank=1\n",
    "            )\n",
    "        elif kernel_type == \"Matern\":\n",
    "            self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=nu, lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(1e-1, 1e10)), num_tasks=num_tasks, rank=1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        \n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 training: 100%|████████████████████████████████████████████████████████████████| 50/50 [01:43<00:00,  2.08s/iter]\n",
      "Evaluating validation data...\n"
     ]
    }
   ],
   "source": [
    "# setup parameters\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "kernel = 'RBF' # 'RBF', 'Matern'\n",
    "num_iter = 50\n",
    "matern_nu = 1.5\n",
    "\n",
    "# cross-validation loop\n",
    "hyperparameter_performance = []\n",
    "for fold, (train_idx, validate_idx) in enumerate(kf.split(X)):\n",
    "\n",
    "    # split the data into train and validate sets\n",
    "    X_train, X_validate = X[train_idx], X[validate_idx]\n",
    "    Y_train, Y_validate = Y_flat[train_idx], Y_flat[validate_idx]\n",
    "    Y_train = Y_train.reshape(X_train.shape[0], -1)\n",
    "    Y_validate = Y_validate.reshape(X_validate.shape[0], -1)\n",
    "\n",
    "    # convert to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "    X_validate_tensor = torch.tensor(X_validate, dtype=torch.float32)\n",
    "    Y_validate_tensor = torch.tensor(Y_validate, dtype=torch.float32)\n",
    "\n",
    "    # initialize the likelihood and model\n",
    "    num_tasks=Y_train.shape[1]\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=Y_train.shape[1])\n",
    "    model = MultitaskGPModel(X_train_tensor, Y_train_tensor, likelihood, num_tasks, kernel_type=kernel, nu=matern_nu)\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()}\n",
    "    ], lr=0.1)\n",
    "\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    for i in tqdm(range(num_iter), desc=f\"Fold {fold + 1} training\", unit=\"iter\", file=sys.stdout):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = -mll(output, Y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation step\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    print('Evaluating validation data...')\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        predictions = likelihood(model(X_validate_tensor))\n",
    "        predicted_mean = predictions.mean\n",
    "        lower, upper = predictions.confidence_region()\n",
    "\n",
    "    # compute evaluation metrics\n",
    "    predicted_mean = predicted_mean.numpy()\n",
    "    rmse = np.sqrt(mean_squared_error(Y_validate, predicted_mean))\n",
    "    mae = mean_absolute_error(Y_validate, predicted_mean)\n",
    "    maxae = np.max(np.abs(Y_validate - predicted_mean))\n",
    "    hyperparameter_performance.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"maxae\": maxae,\n",
    "        \"length_scale\": model.covar_module.data_covar_module.lengthscale.item(),\n",
    "        \"variance\": model.covar_module.task_covar_module.var.detach().numpy()\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, MaxAE: {maxae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCross-validation results:\")\n",
    "rmse_mean, mae_mean, maxae_mean = np.mean([hp[\"rmse\"] for hp in hyperparameter_performance]), \\\n",
    "                                   np.mean([hp[\"mae\"] for hp in hyperparameter_performance]), \\\n",
    "                                   np.mean([hp[\"maxae\"] for hp in hyperparameter_performance])\n",
    "rmse_std, mae_std, maxae_std = np.std([hp[\"rmse\"] for hp in hyperparameter_performance]), \\\n",
    "                               np.std([hp[\"mae\"] for hp in hyperparameter_performance]), \\\n",
    "                               np.std([hp[\"maxae\"] for hp in hyperparameter_performance])\n",
    "\n",
    "print(f\"Mean RMSE: {rmse_mean:.4f} ± {rmse_std:.4f}\")\n",
    "print(f\"Mean MAE: {mae_mean:.4f} ± {mae_std:.4f}\")\n",
    "print(f\"Mean MaxAE: {maxae_mean:.4f} ± {maxae_std:.4f}\")\n",
    "\n",
    "for hp in hyperparameter_performance:\n",
    "    print(f\"Fold {hp['fold']} - Length Scale: {hp['length_scale']}, Variance: {hp['variance']}\")\n",
    "\n",
    "# select best hyperparameters\n",
    "best_fold = min(hyperparameter_performance, key=lambda x: x['rmse'])\n",
    "best_length_scale = best_fold['length_scale']\n",
    "best_variance = best_fold['variance']\n",
    "print(f\"Best fold based on RMSE: {best_fold['fold']}\")\n",
    "print(f\"Best length scale: {best_length_scale}\")\n",
    "print(f\"Best variance: {best_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-train model with optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_samples(param_bounds, n_samples):\n",
    "    n_params = len(param_bounds)\n",
    "    samples = np.random.uniform(\n",
    "        low=[low for low, high in param_bounds],\n",
    "        high=[high for low, high in param_bounds],\n",
    "        size=(n_samples, n_params)\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing dataset\n",
    "n_tests = 20\n",
    "X_test = random_samples(param_bounds, n_tests)\n",
    "Y_test = np.array([\n",
    "    simulator(cl_df[cl_df['datetime'].isin(surrogate_datetime)], params, wn, grouping, mean_vel)\n",
    "    for params in X_test\n",
    "])\n",
    "Y_test_flat = Y_test.reshape(n_tests, Y_test.shape[1] * Y_test.shape[2])\n",
    "\n",
    "# model evaluation\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test_flat, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = likelihood(model(X_test_tensor))\n",
    "    predicted_mean = predictions.mean\n",
    "    lower, upper = predictions.confidence_region()\n",
    "\n",
    "Y_pred_flat_mean = predicted_mean.numpy()\n",
    "Y_pred_mean = Y_pred_flat_mean.reshape(n_tests, Y_test.shape[1], Y_test.shape[2])\n",
    "Y_pred_flat_lower = lower.numpy()\n",
    "Y_pred_lower = Y_pred_flat_lower.reshape(n_tests, Y_test.shape[1], Y_test.shape[2])\n",
    "Y_pred_flat_upper = upper.numpy()\n",
    "Y_pred_upper = Y_pred_flat_upper.reshape(n_tests, Y_test.shape[1], Y_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((Y_test_flat - Y_pred_flat) ** 2))\n",
    "print(f\"Root mean squared error: {mse}\")\n",
    "mae = np.mean(np.abs(Y_test_flat - Y_pred_flat))\n",
    "print(f\"Mean absolute error: {mae}\")\n",
    "maxae = np.max(np.abs(Y_test_flat - Y_pred_flat))\n",
    "print(f\"Maximum absolute error: {maxae}\")\n",
    "\n",
    "# parity plot of surrogate v. simulator\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=Y_test_flat.flatten(),\n",
    "    y=Y_pred_flat_mean.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, opacity=0.6),\n",
    "))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Simulator [mg/L]\",\n",
    "    yaxis_title=\"Surrogate [mg/L]\",\n",
    "    template=\"simple_white\",\n",
    "    width=600,\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# histogram plot of errors\n",
    "errors = (Y_test_flat - Y_pred_flat_mean).flatten()\n",
    "fig = go.Figure(data=[go.Histogram(x=errors, nbinsx=40)])\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Error [mg/L]\",\n",
    "    yaxis_title=f\"Frequency (n={len(errors)})\",\n",
    "    template=\"simple_white\",\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# cdf plot of absolute errors\n",
    "absolute_errors = np.abs(Y_test_flat - Y_pred_flat_mean).flatten()\n",
    "sorted_errors = np.sort(absolute_errors)\n",
    "cdf_values = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sorted_errors,\n",
    "    y=cdf_values,\n",
    "    mode='lines',\n",
    "))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Absolute Error [mg/L]\",\n",
    "    yaxis_title=\"Cumulative distribution\",\n",
    "    template=\"simple_white\",\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sensor_idx = 3\n",
    "sensor_name = cl_simulator.index[selected_sensor_idx]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "actual_data = Y_test[:, selected_sensor_idx, :]\n",
    "predicted_data = Y_pred[:, selected_sensor_idx, :]\n",
    "\n",
    "for exp_idx in range(n_samples-n_train):\n",
    "    color = default_colors[exp_idx % len(default_colors)]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=surrogate_datetime,\n",
    "            y=actual_data[exp_idx, :],\n",
    "            mode='lines',\n",
    "            name=f\"Simulator (Exp {exp_idx + 1})\",\n",
    "            line=dict(color=color, dash='solid'),\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=surrogate_datetime,\n",
    "            y=predicted_data[exp_idx, :],\n",
    "            mode='lines',\n",
    "            name=f\"GP Model (Exp {exp_idx + 1})\",\n",
    "            line=dict(color=color, dash='dash'),\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Chlorine [mg/L]\",\n",
    "    rangemode=\"tozero\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    height=600,  # Fixed height since there's only one plot\n",
    "    template='simple_white',\n",
    "    legend_title_text='',\n",
    "    title=f\"GP model validation sensor {sensor_name}\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loglikelihood score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loglikelihood(sim, cl_df):\n",
    "    \n",
    "#     sensor_data = sensor_model_id('wq')\n",
    "#     bwfl_ids = [sensor for sensor in sensor_data['bwfl_id'].unique() if sensor not in ['BW1', 'BW4']]\n",
    "#     datetime = cl_df['datetime'].unique()[96:]\n",
    "    \n",
    "#     loglikelihood = 0\n",
    "#     for name in bwfl_ids:   \n",
    "#         _sim = sim[name].values\n",
    "#         data = cl_df.loc[cl_df['bwfl_id'] == name, 'mean'].values\n",
    "#         mask = ~np.isnan(sim) & ~np.isnan(data) & (np.arange(len(sim)) >= 96)\n",
    "#         mse += (1 / (len(datetime) * len(bwfl_ids))) * np.sum((sim[mask] - data[mask]) ** 2)\n",
    "    \n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
