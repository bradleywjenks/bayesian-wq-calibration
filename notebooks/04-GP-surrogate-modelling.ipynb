{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a surrogate model of EPANET's water quality solver using Gaussian Process (GP) regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "default_colors = plotly.colors.qualitative.Plotly\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from bayesian_wq_calibration.epanet import build_model, sensor_model_id, epanet_simulator, set_reaction_parameters\n",
    "from bayesian_wq_calibration.calibration import decision_variables_to_dict, generate_samples, setup_gp_model\n",
    "from bayesian_wq_calibration.data import bulk_temp_adjust\n",
    "from bayesian_wq_calibration.constants import TIMESERIES_DIR, RESULTS_DIR\n",
    "from bayesian_wq_calibration.plotting import plot_gp_validation\n",
    "\n",
    "model_dir = RESULTS_DIR / 'wq/gp_models'\n",
    "model_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load operational data for selected sensing period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_period = 16 # 20 calibration events (as at 30 October 2024)\n",
    "wq_sensors_used = 'kiosk + hydrant' # 'kiosk only', 'kiosk + hydrant'\n",
    "demand_resolution = 'wwmd' # 'dma', 'wwmd'\n",
    "try:\n",
    "    flow_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-flow.csv\")\n",
    "    pressure_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-pressure.csv\")\n",
    "    wq_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-wq.csv\", low_memory=False)\n",
    "    cl_df = wq_df[wq_df['data_type'] == 'chlorine']\n",
    "except:\n",
    "    print(f\"Data period {data_period} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate model data period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_days = 5\n",
    "\n",
    "n_total = len(flow_df['datetime'].unique())\n",
    "n_surrogate = surrogate_days * 24 * 4\n",
    "surrogate_range = range(n_surrogate)\n",
    "surrogate_datetime = flow_df['datetime'].unique()[list(surrogate_range)]\n",
    "total_range = range(n_total)\n",
    "total_datetime = flow_df['datetime'].unique()[list(total_range)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_coeff = -0.85 # day^-1 (from bottle tests)\n",
    "field_temp = wq_df[wq_df['data_type'] == 'temperature']['mean'].mean()\n",
    "bulk_coeff = bulk_temp_adjust(bulk_coeff, field_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wall decay grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see notebook `02-pipe-grouping-exploration.ipynb` for details on pipe groups\n",
    "grouping = 'material-age-velocity'\n",
    "\n",
    "# load ga results to get param_mean data\n",
    "ga_results_df = pd.read_excel(RESULTS_DIR / 'wq/ga_calibration.xlsx', sheet_name=grouping)\n",
    "ga_results_df = ga_results_df[(ga_results_df['data_period'] == data_period) & (ga_results_df['demand_resolution'] == demand_resolution) & (ga_results_df['wq_sensors_used'] == wq_sensors_used)]\n",
    "ga_results = ga_results_df[[col for col in ga_results_df.columns if col.startswith('G')]].values[0]\n",
    "\n",
    "grouping_data = {\n",
    "    'single': {\n",
    "        'param_group': ['G0'],\n",
    "        'param_bounds': [(-0.5, 0.0)],\n",
    "        'param_mean': ga_results\n",
    "    },\n",
    "    'material-only': {\n",
    "        'param_group': ['G0', 'G1'],\n",
    "        'param_bounds': [(-1.0, -0.01), (-0.5, -0.01), (-0.15, -0.01)],\n",
    "        'param_mean': ga_results\n",
    "    },\n",
    "    'material-age-diameter': {\n",
    "        'param_group': ['G0', 'G1', 'G2', 'G3', 'G4', 'G5'],\n",
    "        'param_bounds': [(-1.0, -0.01), (-1.0, -0.01), (-0.5, -0.01), (-0.5, -0.01), (-0.2, -0.01)],\n",
    "        'param_mean': ga_results\n",
    "    },\n",
    "    'material-age-velocity': {\n",
    "        'param_group': ['G0', 'G1', 'G2', 'G3', 'G4', 'G5'],\n",
    "        'param_bounds': [(-1.0, -0.01), (-1.0, -0.01), (-0.5, -0.01), (-0.5, -0.01), (-0.2, -0.01)],\n",
    "        'param_mean': ga_results\n",
    "    }\n",
    "}\n",
    "\n",
    "param_data = grouping_data[grouping]\n",
    "param_group = param_data['param_group']\n",
    "param_bounds = param_data['param_bounds']\n",
    "param_mean = param_data['param_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EPANET simulator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build water model using `wntr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = build_model(flow_df[flow_df['datetime'].isin(surrogate_datetime)], pressure_df[pressure_df['datetime'].isin(surrogate_datetime)], cl_df[cl_df['datetime'].isin(surrogate_datetime)], sim_type='chlorine', demand_resolution=demand_resolution, bulk_coeff=bulk_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get mean velocities (for 'material-velocity' grouping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define simualtor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(cl_df, params, wn, grouping):\n",
    "    \"\"\"Run EPANET simulation with given parameters.\"\"\"\n",
    "    wall_coeffs = decision_variables_to_dict(grouping, params)\n",
    "    _wn = set_reaction_parameters(wn, grouping, wall_coeffs, bulk_coeff)\n",
    "    \n",
    "    sim_results = epanet_simulator(_wn, 'chlorine', cl_df)\n",
    "    cl_sim = sim_results.chlorine\n",
    "    \n",
    "    sensor_data = sensor_model_id('wq')\n",
    "    cl_sim = cl_sim[sensor_data['model_id'].unique()]\n",
    "    name_mapping = sensor_data.set_index('model_id')['bwfl_id'].to_dict()\n",
    "    cl_sim = cl_sim.rename(columns=name_mapping)\n",
    "\n",
    "    cl_sim = cl_sim.T\n",
    "    cl_sim.columns = [f't_{idx+1}' for idx in range(cl_sim.shape[1])]\n",
    "    cl_sim = cl_sim.drop(index=['BW1', 'BW4'], errors='ignore')  # remove inlet sensors\n",
    "    \n",
    "    return cl_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_simulator = simulator(cl_df[cl_df['datetime'].isin(surrogate_datetime)], param_mean, wn, grouping)\n",
    "sensor_names = cl_simulator.index.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design of experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `generate_samples` function, which takes the following inputs:\n",
    "- `sampling_method` (default = latin hypercube sampling)\n",
    "- `dist_type` (default = truncated normal)\n",
    "- `rel_uncertainty` (default = 50% of parameter mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_idx = 3\n",
    "n_samples = [10, 25, 50, 100, 200]\n",
    "\n",
    "X = generate_samples(param_mean, param_bounds, param_group, n_samples[n_samples_idx], sampling_method='lhs', dist_type='triangle', rel_uncertainty=0.75, plot=True)\n",
    "Y = np.array([\n",
    "    simulator(cl_df[cl_df['datetime'].isin(surrogate_datetime)], params, wn, grouping)\n",
    "    for params in X\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation of GP regression training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure using five-fold cross-validation and `scikit-learn` modules. The following kernel's can be used:\n",
    "- Radial basis function (RBF)\n",
    "- Matern\n",
    "- Rational quadratic\n",
    "\n",
    "Note: a separate GP is trained for each of the **7** sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'BW2' # 'BW2', 'BW3', 'BW5', 'BW6', 'BW7', 'BW9', 'BW12'\n",
    "s = np.where(sensor_names == sensor)[0][0]\n",
    "Y_s = Y[:, s, :].reshape(Y.shape[0], Y.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for training and testing\n",
    "n_0 = round(0.8*len(X))\n",
    "X_train = X[:n_0]\n",
    "Y_train = Y_s[:n_0]\n",
    "X_test = X[n_0:]\n",
    "Y_test = Y_s[n_0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "kernel_type = 'RBF'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_folds = 5\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # cross-validation loop\n",
    "# hyperparameter_performance = []\n",
    "# for fold, (train_idx, validate_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    \n",
    "#     X_cv_train, X_cv_validate = X_train_scaled[train_idx], X_train_scaled[validate_idx]\n",
    "#     Y_cv_train, Y_cv_validate = Y_train[train_idx], Y_train[validate_idx]\n",
    "    \n",
    "#     gp = setup_gp_model(X_train.shape[1], kernel_type=kernel_type)\n",
    "#     gp.fit(X_cv_train, Y_cv_train)\n",
    "    \n",
    "#     Y_cv_pred = gp.predict(X_cv_validate)\n",
    "    \n",
    "#     # performance metrics\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_cv_validate, Y_cv_pred))\n",
    "#     mae = mean_absolute_error(Y_cv_validate, Y_cv_pred)\n",
    "#     maxae = np.max(np.abs(Y_cv_validate - Y_cv_pred))\n",
    "#     hyperparameter_performance.append({\n",
    "#         \"fold\": fold + 1,\n",
    "#         \"rmse\": rmse,\n",
    "#         \"mae\": mae,\n",
    "#         \"maxae\": maxae,\n",
    "#         \"length_scale\": gp.kernel_.get_params()['k2__length_scale'],\n",
    "#         \"variance\": gp.kernel_.get_params()['k1__constant_value']\n",
    "#     })\n",
    "    \n",
    "#     print(f\"Fold {fold + 1} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, MaxAE: {maxae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train GP model with entire dataset (or load an existing model).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'gp_{data_period}_{sensor}_{demand_resolution}_{surrogate_days}-day_sim.joblib'\n",
    "model_path = model_dir / filename\n",
    "train_model = True\n",
    "\n",
    "if not train_model:\n",
    "    try:\n",
    "        print(f\"Attempting to load saved model: {filename}\")\n",
    "        model_data = joblib.load(model_path)\n",
    "        gp = model_data['gp_model']\n",
    "        scaler = model_data['scaler']\n",
    "        print(f\"Successfully loaded saved model: {filename}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"No saved model found at {model_path}\")\n",
    "        print(\"Set `train_model=True` and re-run code block.\")\n",
    "else:\n",
    "        \n",
    "    # train model using entire dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    gp = setup_gp_model(X_train.shape[1], kernel_type='RBF')\n",
    "    gp.fit(X_train_scaled, Y_train)\n",
    "    \n",
    "    # save newly trained model\n",
    "    model_save = {\n",
    "        'gp_model': gp,\n",
    "        'scaler': scaler,\n",
    "        'kernel_params': gp.kernel_.get_params(),\n",
    "        'sensor_name': sensor\n",
    "    }\n",
    "    \n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(model_save, model_path)\n",
    "    print(f\"New model trained and saved to: {model_path}\")\n",
    "\n",
    "# print GP model parameters\n",
    "print(\"GP model parameters:\")\n",
    "print(gp.kernel_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GP model testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred, Y_std = gp.predict(X_test_scaled, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "maxae = np.max(np.abs(Y_test - Y_pred))\n",
    "r2 = 1 - np.sum((Y_test - Y_pred) ** 2) / np.sum((Y_test - Y_test.mean()) ** 2)\n",
    "\n",
    "print(f\"\\nPerformance metrics (test dataset).\")\n",
    "print(f\"Root mean squared error: {rmse:.4f}\")\n",
    "print(f\"Mean absolute error: {mae:.4f}\")\n",
    "print(f\"Maximum absolute error: {maxae}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# visualization\n",
    "\n",
    "# parity plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=Y_test.flatten(),\n",
    "    y=Y_pred.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, opacity=0.6),\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"EPANET vs GP\",\n",
    "    xaxis_title=\"EPANET simulator [mg/L]\",\n",
    "    yaxis_title=\"GP surrogate [mg/L]\",\n",
    "    template=\"simple_white\",\n",
    "    width=550,\n",
    "    height=450\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# error distribution\n",
    "errors = (Y_test - Y_pred).flatten()\n",
    "fig = go.Figure(data=[go.Histogram(x=errors, nbinsx=40)])\n",
    "fig.update_layout(\n",
    "    title=\"Error distribution\",\n",
    "    xaxis_title=\"Error [mg/L]\",\n",
    "    yaxis_title=f\"Frequency (n={len(errors)})\",\n",
    "    template=\"simple_white\",\n",
    "    width=550,\n",
    "    height=450\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# cdf plot of absolute errors\n",
    "absolute_errors = np.abs(Y_test - Y_pred).flatten()\n",
    "sorted_errors = np.sort(absolute_errors)\n",
    "cdf_values = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sorted_errors,\n",
    "    y=cdf_values,\n",
    "    mode='lines',\n",
    "    line=dict(width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cumulative distribution of absolute errors',\n",
    "    xaxis_title='Absolute error [mg/L]',\n",
    "    yaxis_title='Cumulative probability',\n",
    "    template='simple_white',\n",
    "    width=550,\n",
    "    height=450,\n",
    "    showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp_validation(Y_test, Y_pred, Y_std, surrogate_datetime[surrogate_range], sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
