{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3720c0b-385a-49a1-8c62-12a95cebf5a7",
   "metadata": {},
   "source": [
    "# Model calibration via MCMC sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb27f6e-3e11-4c8b-8d46-65900bb50c8f",
   "metadata": {},
   "source": [
    "This notebook implements the Metropolis-Hastings MCMC algorithm via the `pymc` package to infer posterior distributions of wall decay coefficients in the Bristol Water Field Lab. It employs a GP surrogate model in place of the (expensive) EPANET water quality simulator (see `04-GP_surrogate_modelling.ipynb`) and field data organized in `01-data-period-features.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1213241-ec28-4125-901e-cd798cf49654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.stats import truncnorm, triang, uniform, norm\n",
    "from tqdm.notebook import tqdm\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph.basic import Apply\n",
    "from pytensor.graph.op import Op\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.colors\n",
    "default_colors = plotly.colors.qualitative.Plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bayesian_wq_calibration.constants import TIMESERIES_DIR, RESULTS_DIR\n",
    "from bayesian_wq_calibration.calibration import decision_variables_to_dict\n",
    "from bayesian_wq_calibration.epanet import build_model, epanet_simulator, sensor_model_id\n",
    "from bayesian_wq_calibration.data import bulk_temp_adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f755aa-8a82-41f6-818f-8085643b185a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a9763-df5c-4767-9f4a-7a0bc75393bb",
   "metadata": {},
   "source": [
    "Load operational data for selected sensing period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0c6b6-a446-4593-b51c-e8b8f1159eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_period = 18 # 20 calibration events (as at 30 October 2024)\n",
    "wq_sensors_used = 'kiosk + hydrant' # 'kiosk only', 'kiosk + hydrant'\n",
    "demand_resolution = 'wwmd' # 'dma', 'wwmd'\n",
    "\n",
    "try:\n",
    "    wq_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-wq.csv\", low_memory=False)\n",
    "    cl_df = wq_df[wq_df['data_type'] == 'chlorine']\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Data period {data_period} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e374ef2-1f90-4f38-a9b8-05701ead3a33",
   "metadata": {},
   "source": [
    "Set modelling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddca091-3870-478c-b1c9-07e070edd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_days = 5  # must match number of days used to train GP model\n",
    "time_steps = sim_days * 24 * 4\n",
    "datetime_vals = cl_df['datetime'].unique()\n",
    "datetime_subset = datetime_vals[:time_steps]\n",
    "cl_df = cl_df[cl_df['datetime'].isin(datetime_subset)]\n",
    "\n",
    "source_ids = ['BW1', 'BW4']\n",
    "kiosk_ids = ['BW1', 'BW2', 'BW4', 'BW5', 'BW9', 'BW12']\n",
    "bad_ids = ['BW7']\n",
    "if wq_sensors_used == 'kiosk only':\n",
    "    cl_df = cl_df[(cl_df['bwfl_id'].isin(kiosk_ids)) & (~cl_df['bwfl_id'].isin(source_ids)) & (~cl_df['bwfl_id'].isin(bad_ids))]\n",
    "else:\n",
    "    cl_df = cl_df[(~cl_df['bwfl_id'].isin(source_ids)) & (~cl_df['bwfl_id'].isin(bad_ids))]\n",
    "sensor_names = cl_df['bwfl_id'].unique()\n",
    "demand_resolution = 'wwmd' # 'dma', 'wwmd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1b897-f5bf-4098-a450-2a54b4d451c1",
   "metadata": {},
   "source": [
    "Bulk decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5aed59-1a07-4c5b-b3a2-802b28c28506",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_coeff = -0.7 # day^-1 (from bottle tests)\n",
    "field_temp = wq_df[wq_df['data_type'] == 'temperature']['mean'].mean()\n",
    "bulk_coeff = bulk_temp_adjust(bulk_coeff, field_temp)\n",
    "bulk_uncertainty = 0.05\n",
    "bulk_coeff_bounds = (bulk_coeff*(1+bulk_uncertainty*5), bulk_coeff*(1-bulk_uncertainty*5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b9a3d-6e40-4a93-86ab-25b2f1ce6119",
   "metadata": {},
   "source": [
    "Wall decay grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3520b-89fa-4c93-82fa-747c9460dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see notebook `02-pipe-grouping-exploration.ipynb` for details on pipe groups\n",
    "grouping = 'material-age-velocity'\n",
    "\n",
    "# load ga results to get param_mean data\n",
    "ga_results_df = pd.read_excel(RESULTS_DIR / 'wq/ga_calibration.xlsx', sheet_name=grouping)\n",
    "ga_results_df = ga_results_df[(ga_results_df['data_period'] == data_period) & (ga_results_df['demand_resolution'] == demand_resolution) & (ga_results_df['wq_sensors_used'] == wq_sensors_used)]\n",
    "ga_results = ga_results_df[[col for col in ga_results_df.columns if col.startswith('G')]].values[0]\n",
    "\n",
    "grouping_data = {\n",
    "    'single': {\n",
    "        'param_group': ['B', 'W0'],\n",
    "        'param_bounds': [bulk_coeff_bounds, (-0.5, 0.0)],\n",
    "        'param_mean': np.concatenate([[bulk_coeff], ga_results])\n",
    "    },\n",
    "    'material-only': {\n",
    "        'param_group': ['B', 'W0', 'W1'],\n",
    "        'param_bounds': [bulk_coeff_bounds, (-1.0, -0.01), (-0.5, -0.01), (-0.15, -0.01)],\n",
    "        'param_mean': np.concatenate([[bulk_coeff], ga_results])\n",
    "    },\n",
    "    'material-age-diameter': {\n",
    "        'param_group': ['B', 'W0', 'W1', 'W2', 'W3', 'W4'],\n",
    "        'param_bounds': [bulk_coeff_bounds, (-1.0, -0.01), (-1.0, -0.01), (-0.5, -0.01), (-0.5, -0.01), (-0.2, -0.01)],\n",
    "        'param_mean': np.concatenate([[bulk_coeff], ga_results])\n",
    "    },\n",
    "    'material-age-velocity': {\n",
    "        'param_group': ['B', 'W0', 'W1', 'W2', 'W3', 'W4'],\n",
    "        'param_bounds': [bulk_coeff_bounds, (-1.0, -0.01), (-1.0, -0.01), (-0.5, -0.01), (-0.5, -0.01), (-0.2, -0.01)],\n",
    "        'param_mean': np.concatenate([[bulk_coeff], ga_results])\n",
    "    }\n",
    "}\n",
    "\n",
    "param_data = grouping_data[grouping]\n",
    "param_group = param_data['param_group']\n",
    "param_bounds = param_data['param_bounds']\n",
    "param_mean = param_data['param_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd64ba9-b407-42a2-bee4-fbb1b7ab17ad",
   "metadata": {},
   "source": [
    "### MCMC algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39ffd4-af71-4cc8-8e69-099a0f3bcfb1",
   "metadata": {},
   "source": [
    "Define functions here. (To be moved to `src` folder at a later date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7217b8-475b-4cb5-bdf0-c42f86be42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gp_models(sensor_names, demand_resolution, sim_days):\n",
    "    \"\"\"Load saved GP models\"\"\"\n",
    "    model_dir = RESULTS_DIR / 'wq/gp_models'\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    gp_models = {}\n",
    "    \n",
    "    for sensor in sensor_names:\n",
    "        filename = f'gp_{data_period}_{sensor}_{demand_resolution}_{sim_days}-day_sim.joblib'\n",
    "        model_path = model_dir / filename\n",
    "        gp_models[sensor] = joblib.load(model_path)\n",
    "    \n",
    "    return gp_models\n",
    "\n",
    "\n",
    "class GPLikelihood(Op):\n",
    "    \"\"\"Custom PyTensor Op for GP likelihood calculation\"\"\"\n",
    "    \n",
    "    def __init__(self, gp_models, cl_df):\n",
    "        self.gp_models = gp_models\n",
    "        self.cl_df = cl_df\n",
    "        \n",
    "    def make_node(self, theta, noise_std):\n",
    "        # Convert inputs to tensors\n",
    "        theta = pt.as_tensor_variable(theta)\n",
    "        noise_std = pt.as_tensor_variable(noise_std)\n",
    "        \n",
    "        # Define output type (scalar log likelihood)\n",
    "        return Apply(self, [theta, noise_std], [pt.scalar()])\n",
    "    \n",
    "    def perform(self, node, inputs, outputs):\n",
    "        theta, noise_std = inputs\n",
    "        \n",
    "        # Calculate log likelihood\n",
    "        log_l = 0\n",
    "        theta_1d = theta.reshape(1, -1)\n",
    "        \n",
    "        for sensor in self.gp_models:\n",
    "            sensor_data = self.cl_df.loc[self.cl_df['bwfl_id'] == sensor, 'mean'].values\n",
    "            mask = ~np.isnan(sensor_data) & (np.arange(len(sensor_data)) >= 96)\n",
    "            observed = sensor_data[mask]\n",
    "            \n",
    "            theta_scaled = self.gp_models[sensor]['scaler'].transform(theta_1d)\n",
    "            predicted = self.gp_models[sensor]['gp_model'].predict(theta_scaled).T.ravel()[mask]\n",
    "            \n",
    "            # Calculate log likelihood using normal distribution\n",
    "            log_l += norm.logpdf(observed, predicted, noise_std).sum()\n",
    "        \n",
    "        outputs[0][0] = np.array(log_l, dtype='float64')\n",
    "\n",
    "\n",
    "def custom_loglike(observed, theta, noise_std, gp_likelihood_op):\n",
    "    \"\"\"Custom likelihood function for PyMC\"\"\"\n",
    "    return gp_likelihood_op(theta, noise_std)\n",
    "\n",
    "\n",
    "def mcmc_pymc(gp_models, cl_df, prior_params, n_iter=10000, noise_std=0.1, burn_in=1000):\n",
    "    \"\"\"Run MCMC using PyMC with custom likelihood\"\"\"\n",
    "    \n",
    "    # create GP likelihood op\n",
    "    gp_likelihood_op = GPLikelihood(gp_models, cl_df)\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        \n",
    "        # define priors\n",
    "        theta = [\n",
    "            pm.TruncatedNormal(\n",
    "                f'theta_{i}',\n",
    "                mu=prior['mu'],\n",
    "                sigma=abs(prior['mu'] * prior['rel_uncertainty']),\n",
    "                lower=prior['lower_bound'],\n",
    "                upper=prior['upper_bound']\n",
    "            )\n",
    "            for i, prior in enumerate(prior_params)\n",
    "        ]\n",
    "        \n",
    "        # stack parameters into a vector\n",
    "        theta_vector = pt.stack(theta)\n",
    "        \n",
    "        # add likelihood using CustomDist\n",
    "        likelihood = pm.CustomDist(\n",
    "            'likelihood',\n",
    "            theta_vector,\n",
    "            noise_std,\n",
    "            observed=np.array([0.0]),  # Dummy observed value\n",
    "            logp=lambda observed, theta, noise_std: custom_loglike(\n",
    "                observed, theta, noise_std, gp_likelihood_op\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # run sampling with Metropolis\n",
    "        idata = pm.sample(\n",
    "            draws=n_iter,\n",
    "            tune=burn_in,\n",
    "            step=pm.Metropolis(tune=True),\n",
    "            return_inferencedata=True,\n",
    "            progressbar=True,\n",
    "            chains=4,\n",
    "            # cores=1\n",
    "        )\n",
    "    \n",
    "    return idata\n",
    "\n",
    "def plot_mcmc_traces(trace, param_names=None, text_size=12, v_space=0.15, h_space=0.2):\n",
    "    \"\"\"create interactive trace plots using plotly\"\"\"\n",
    "    \n",
    "    # if parameter names not provided, extract from trace\n",
    "    if param_names is None:\n",
    "        param_names = [var for var in trace.posterior.variables if var.startswith('theta')]\n",
    "    \n",
    "    n_params = len(param_names)\n",
    "    \n",
    "    # create subplots with corrected titles\n",
    "    subplot_titles = []\n",
    "    for i in range(n_params):\n",
    "        subplot_titles.extend([f\"theta_{i} distribution\", f\"theta_{i} trace\"])\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=n_params, \n",
    "        cols=2,\n",
    "        subplot_titles=subplot_titles,\n",
    "        vertical_spacing=v_space,  # controllable vertical spacing\n",
    "        horizontal_spacing=h_space  # controllable horizontal spacing\n",
    "    )\n",
    "    \n",
    "    # add traces for each parameter\n",
    "    for i, param in enumerate(param_names, 1):\n",
    "        # get values for this parameter\n",
    "        values = trace.posterior[param].values.flatten()\n",
    "        iterations = np.arange(len(values))\n",
    "        \n",
    "        # add histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=values,\n",
    "                name=f\"{param} Distribution\",\n",
    "                nbinsx=50,\n",
    "                showlegend=False,\n",
    "                marker_color=default_colors[0]\n",
    "            ),\n",
    "            row=i, col=1\n",
    "        )\n",
    "        \n",
    "        # add trace plot in grey\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=iterations,\n",
    "                y=values,\n",
    "                mode='lines',\n",
    "                name=f\"{param} Trace\",\n",
    "                showlegend=False,\n",
    "                line=dict(color=default_colors[0])\n",
    "            ),\n",
    "            row=i, col=2\n",
    "        )\n",
    "    \n",
    "    # update layout\n",
    "    fig.update_layout(\n",
    "        height=300 * n_params,\n",
    "        width=1000,\n",
    "        title_text=\"MCMC summary plots\",\n",
    "        showlegend=False,\n",
    "        template='simple_white'\n",
    "    )\n",
    "    \n",
    "    # update font sizes for subplot titles\n",
    "    for annotation in fig['layout']['annotations']:\n",
    "       annotation['font'] = dict(size=text_size + 1)  # subplot titles slightly larger than base\n",
    "    \n",
    "    # update axes labels and tick font sizes\n",
    "    for i in range(n_params):\n",
    "       # distribution plot\n",
    "       fig.update_xaxes(\n",
    "           title_text=\"Value\",\n",
    "           title_font=dict(size=text_size),\n",
    "           tickfont=dict(size=text_size),\n",
    "           row=i+1, col=1\n",
    "       )\n",
    "       fig.update_yaxes(\n",
    "           title_text=\"Count\",\n",
    "           title_font=dict(size=text_size),\n",
    "           tickfont=dict(size=text_size),\n",
    "           row=i+1, col=1\n",
    "       )\n",
    "       \n",
    "       # trace plot\n",
    "       fig.update_xaxes(\n",
    "           title_text=\"Iteration\",\n",
    "           title_font=dict(size=text_size),\n",
    "           tickfont=dict(size=text_size),\n",
    "           row=i+1, col=2\n",
    "       )\n",
    "       fig.update_yaxes(\n",
    "           title_text=\"Value\",\n",
    "           title_font=dict(size=text_size),\n",
    "           tickfont=dict(size=text_size),\n",
    "           row=i+1, col=2\n",
    "       )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def propose_first_parameter(idx, current_param, dist_type, bulk_uncertainty, wall_uncertainty, param_bounds):\n",
    "\n",
    "    sample = np.random.random()\n",
    "    \n",
    "    if idx == 0:\n",
    "        sigma = abs(current_param * bulk_uncertainty)\n",
    "        return norm.ppf(sample, loc=current_param, scale=sigma)\n",
    "    else:\n",
    "        sigma = abs(current_param * wall_uncertainty)\n",
    "        lb = param_bounds[0]\n",
    "        ub = param_bounds[1]\n",
    "\n",
    "        if dist_type == 'truncated normal':\n",
    "            a = (lb - current_param) / sigma\n",
    "            b = (ub - current_param) / sigma\n",
    "            return truncnorm.ppf(sample, a=a, b=b, loc=current_param, scale=sigma)\n",
    "            \n",
    "        elif dist_type == 'triangle':\n",
    "            c = (current_param - lb) / (ub - lb)\n",
    "            return triang.ppf(sample, c=c, loc=lb, scale=ub-lb)\n",
    "            \n",
    "        elif dist_type == 'uniform':\n",
    "            return uniform.ppf(sample, loc=lb, scale=ub-lb)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distribution type: {dist_type}\")\n",
    "\n",
    "\n",
    "def propose_next_parameter(current_param, step_size):\n",
    "\n",
    "    sample = np.random.random()\n",
    "    sigma = abs(current_param * step_size)\n",
    "    return norm.ppf(sample, loc=current_param, scale=sigma)\n",
    "\n",
    "    \n",
    "def log_prior(theta, priors):\n",
    "    \"\"\"Calculate log prior for the given parameters.\"\"\"\n",
    "    log_p = 0\n",
    "    \n",
    "    for i, prior in enumerate(priors):\n",
    "        if i == 0:\n",
    "            sigma = max(abs(prior['mu'] * prior['bulk_uncertainty']), 1e-6)\n",
    "            log_p += norm.logpdf(theta[i], loc=prior['mu'], scale=sigma)\n",
    "        else:\n",
    "            sigma = max(abs(prior['mu'] * prior['wall_uncertainty']), 1e-6)\n",
    "            if prior['dist_type'] == 'truncated normal':\n",
    "                a = (prior['lower_bound'] - prior['mu']) / sigma\n",
    "                b = (prior['upper_bound'] - prior['mu']) / sigma\n",
    "                log_p += truncnorm.logpdf(theta[i], a=a, b=b, loc=prior['mu'], scale=sigma)\n",
    "            elif prior['dist_type'] == 'triangle':\n",
    "                c = (prior['mu'] - prior['lower_bound']) / (prior['upper_bound'] - prior['lower_bound'])\n",
    "                log_p += triang.logpdf(theta[i], c=c, loc=prior['lower_bound'], scale=prior['upper_bound'] - prior['lower_bound'])\n",
    "            elif prior['dist_type'] == 'uniform':\n",
    "                log_p += uniform.logpdf(theta[i], loc=prior['lower_bound'], scale=prior['upper_bound'] - prior['lower_bound'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported distribution type: {prior['dist_type']}\")\n",
    "    return log_p\n",
    "\n",
    "    \n",
    "def log_likelihood(theta):\n",
    "        \"\"\"Calculate log likelihood across all sensors\"\"\"\n",
    "        log_l = 0\n",
    "        theta_1d = theta.reshape(1, -1)\n",
    "        \n",
    "        for sensor in gp_models:\n",
    "            sensor_data = cl_df.loc[cl_df['bwfl_id'] == sensor, 'mean'].values\n",
    "            mask = ~np.isnan(sensor_data) & (np.arange(len(sensor_data)) >= 96)\n",
    "            observed = sensor_data[mask]\n",
    "            \n",
    "            theta_scaled = gp_models[sensor]['scaler'].transform(theta_1d)\n",
    "            predicted = gp_models[sensor]['gp_model'].predict(theta_scaled).T.ravel()[mask]\n",
    "            \n",
    "            log_l += norm.logpdf(observed, predicted, noise_std).sum()\n",
    "        return log_l\n",
    "    \n",
    "\n",
    "def gelman_rubin(chains, burn_in):\n",
    "    \n",
    "    n_chains, n_iter, n_params = chains.shape\n",
    "    chains = chains[:, burn_in:, :]  # Remove burn-in\n",
    "    \n",
    "    # calculate within-chain and between-chain variance\n",
    "    chain_means = np.mean(chains, axis=1)  # Shape: (n_chains, n_params)\n",
    "    chain_vars = np.var(chains, axis=1, ddof=1)  # Shape: (n_chains, n_params)\n",
    "    \n",
    "    # overall mean for each parameter\n",
    "    param_means = np.mean(chain_means, axis=0)  # Shape: (n_params,)\n",
    "    \n",
    "    # between-chain variance\n",
    "    B = n_iter * np.var(chain_means, axis=0, ddof=1)  # Shape: (n_params,)\n",
    "    \n",
    "    # within-chain variance\n",
    "    W = np.mean(chain_vars, axis=0)  # Shape: (n_params,)\n",
    "    \n",
    "    # calculate PSRF (potential scale reduction factor)\n",
    "    V = ((n_iter - 1) / n_iter) * W + ((n_chains + 1) / (n_chains * n_iter)) * B\n",
    "    R = np.sqrt(V / W)\n",
    "    \n",
    "    return R\n",
    "    \n",
    "\n",
    "def metropolis_hastings(gp_models, cl_df, priors, step_size=0.1, n_chains=4, n_iter=10000, noise_std=0.1, burn_in=500):\n",
    "    \"\"\"Metropolis-Hastings implementation with multiple chains\"\"\"\n",
    "    \n",
    "    n_params = len(priors)\n",
    "    chains = np.zeros((n_chains, n_iter, n_params))\n",
    "    log_posteriors = np.zeros((n_chains, n_iter))\n",
    "    acceptance_rates = np.zeros(n_chains)\n",
    "    \n",
    "    # run separate chains\n",
    "    for chain_idx in range(n_chains):\n",
    "        print(f\"\\nRunning chain {chain_idx + 1}/{n_chains}\")\n",
    "        \n",
    "        # initialize different starting point for each chain\n",
    "        current_theta = np.array([\n",
    "            propose_first_parameter(\n",
    "                idx,\n",
    "                prior['mu'], \n",
    "                prior['dist_type'],\n",
    "                prior['bulk_uncertainty'],\n",
    "                prior['wall_uncertainty'],\n",
    "                (prior['lower_bound'], prior['upper_bound'])\n",
    "            )\n",
    "            for idx, prior in enumerate(priors)\n",
    "        ])\n",
    "        \n",
    "        # calculate initial log posterior\n",
    "        current_log_posterior = log_prior(current_theta, priors) + log_likelihood(current_theta)\n",
    "        \n",
    "        # Run MCMC\n",
    "        accepts = 0\n",
    "        pbar = tqdm(range(n_iter), desc=f\"Chain {chain_idx + 1} - Acceptance rate: 0.000\")\n",
    "        \n",
    "        for i in pbar:\n",
    "            \n",
    "            # propose new parameters\n",
    "            proposal = np.array([\n",
    "                propose_next_parameter(\n",
    "                    current_theta[j],\n",
    "                    step_size,\n",
    "                )\n",
    "                for j in range(len(current_theta))\n",
    "            ])\n",
    "                \n",
    "            # calculate log likelihood ratio\n",
    "            proposal_log_posterior = log_prior(proposal, priors) + log_likelihood(proposal)\n",
    "            log_ratio = proposal_log_posterior - current_log_posterior\n",
    "            \n",
    "            # accept/reject\n",
    "            if np.log(np.random.random()) < log_ratio:\n",
    "                current_theta = proposal\n",
    "                current_log_posterior = proposal_log_posterior\n",
    "                accepts += 1\n",
    "            \n",
    "            # Store current state\n",
    "            chains[chain_idx, i] = current_theta\n",
    "            log_posteriors[chain_idx, i] = current_log_posterior\n",
    "            \n",
    "            # Update progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                accept_rate = (accepts / (i + 1)) * 100\n",
    "                pbar.set_description(f\"Chain {chain_idx + 1} - Acceptance rate: {accept_rate:.3f}\")\n",
    "        \n",
    "        acceptance_rates[chain_idx] = accepts/n_iter\n",
    "    \n",
    "    # print summary statistics and convergence diagnostics\n",
    "    print(\"\\nChain Summary:\")\n",
    "    for i in range(n_chains):\n",
    "        print(f\"\\nChain {i+1} acceptance rate: {acceptance_rates[i]:.3f}\")\n",
    "    \n",
    "    print(\"\\nParameter Summary (post burn-in):\")\n",
    "    combined_samples = chains[:, burn_in:, :].reshape(-1, n_params)\n",
    "    for j in range(n_params):\n",
    "        param_samples = combined_samples[:, j]\n",
    "        print(f\"\\nParameter {j+1}:\")\n",
    "        print(f\"Mean: {np.mean(param_samples):.3f}\")\n",
    "        print(f\"Std: {np.std(param_samples):.3f}\")\n",
    "        print(f\"95% CI: [{np.percentile(param_samples, 2.5):.3f}, {np.percentile(param_samples, 97.5):.3f}]\")\n",
    "    \n",
    "    # calculate and print Gelman-Rubin statistics\n",
    "    r_stats = gelman_rubin(chains, burn_in)\n",
    "    print(\"\\nGelman-Rubin Statistics:\")\n",
    "    for j in range(n_params):\n",
    "        print(f\"Parameter {j+1}: {r_stats[j]:.3f}\")\n",
    "        if r_stats[j] > 1.1:\n",
    "            print(\"WARNING: R > 1.1 suggests poor convergence\")\n",
    "    \n",
    "    return chains, log_likelihoods, r_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233ce09-0850-414f-a937-70de04398b49",
   "metadata": {},
   "source": [
    "Run MCMC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71081210-5177-46de-9132-1a0fb42fcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GP models\n",
    "gp_models = load_gp_models(sensor_names, demand_resolution, sim_days)\n",
    "\n",
    "# set up prior parameters\n",
    "prior_params = []\n",
    "wall_uncertainty = 0.5\n",
    "\n",
    "prior_params = [{\n",
    "    'dist_type': 'truncated normal',  # 'truncated normal', 'triangle', or 'uniform'\n",
    "    'mu': mean,\n",
    "    'bulk_uncertainty': bulk_uncertainty,\n",
    "    'wall_uncertainty': wall_uncertainty,\n",
    "    'lower_bound': bounds[0],\n",
    "    'upper_bound': bounds[1]\n",
    "} for mean, bounds in zip(param_mean, param_bounds)]\n",
    "\n",
    "\n",
    "# MCMC settings\n",
    "n_iter = 10000\n",
    "noise_std = 0.1\n",
    "burn_in = 1000\n",
    "n_chains = 4\n",
    "step_size = 0.1\n",
    "\n",
    "# run MCMC\n",
    "# trace = mcmc_pymc(gp_models, cl_df, prior_params, n_iter=n_iter, noise_std=noise_std, burn_in=burn_in)\n",
    "trace, log_likelihoods, r_stats = metropolis_hastings(gp_models, cl_df, prior_params, noise_std=noise_std, n_chains=n_chains, burn_in=burn_in, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee25e2-e901-42a6-8a9b-fa3c15be88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior distribution summary\n",
    "summary = az.summary(trace)\n",
    "print(\"\\nParameter summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b4d7e-c311-41fc-9eac-cb67cb7b03a3",
   "metadata": {},
   "source": [
    "### Results plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9678d-3fa6-42b7-bfa3-576aa0ac271a",
   "metadata": {},
   "source": [
    "Plotting via the `arviz` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b5d56-cf57-443d-a807-13b84bb76982",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8937e72-e0ae-46e6-ba1f-ca937f4f4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052a25e-8e0f-434e-8967-291464c81f29",
   "metadata": {},
   "source": [
    "Manual plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491e814-7111-46b4-a1e9-fc3498f89733",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mcmc_traces(trace, text_size=20, v_space=0.1, h_space=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a93d12-ba38-4bd9-9701-a3090380c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55725da8-225d-47b4-be63-f8ab6778f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_idx = 0\n",
    "\n",
    "# posterior distribution\n",
    "fig = go.Figure(data=[go.Histogram(x=trace[0, burn_in:, theta_idx], nbinsx=50, marker_color='grey')])\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=f\"{param_group[theta_idx]}\",\n",
    "        x=0.5,\n",
    "        y=0.99,\n",
    "        font=dict(size=60)\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"θ [m/d]\",\n",
    "        title_font=dict(size=60),\n",
    "        tickfont=dict(size=60),\n",
    "        tickangle=45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        visible=False\n",
    "    ),\n",
    "    template=\"simple_white\",\n",
    "    width=1100,\n",
    "    height=950\n",
    ")\n",
    "fig.show()\n",
    "if wq_sensors_used == 'kiosk only':\n",
    "    fig.write_image(RESULTS_DIR / f'wq/posterior_dists/{param_group[theta_idx]}_{data_period}_{demand_resolution}_kiosk-only_{sim_days}-day_sim.png')\n",
    "else:\n",
    "    fig.write_image(RESULTS_DIR / f'wq/posterior_dists/{param_group[theta_idx]}_{data_period}_{demand_resolution}_kiosk+hydrant_{sim_days}-day_sim.png')\n",
    "\n",
    "# # trace plot\n",
    "# fig = go.Figure(data=go.Scatter(\n",
    "#     x=np.arange(0,len(trace[burn_in:, theta_idx])-1),\n",
    "#     y=trace[burn_in:, theta_idx],\n",
    "#     mode='markers',\n",
    "#     marker=dict(size=6, opacity=0.6),\n",
    "# ))\n",
    "# fig.update_layout(\n",
    "#     # title=\"EPANET vs GP\",\n",
    "#     # xaxis_title=\"EPANET simulator [mg/L]\",\n",
    "#     # yaxis_title=\"GP surrogate [mg/L]\",\n",
    "#     template=\"simple_white\",\n",
    "#     width=550,\n",
    "#     height=450\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de85382-75df-446f-829d-90b0ffd4d2a2",
   "metadata": {},
   "source": [
    "### Verify calibration performance (goodness of fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835f39b-1598-4379-bb43-8b1caa3f2bf5",
   "metadata": {},
   "source": [
    "Get mean wall decay coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de028528-e323-408e-9d0e-f9da0ad64c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean parameters\n",
    "wall_coeffs_mean = trace.mean(axis=0)\n",
    "wall_coeffs_mean = decision_variables_to_dict(grouping, wall_coeffs_mean)\n",
    "print(wall_coeffs_mean)\n",
    "\n",
    "# compute 2.5th percentile parameters\n",
    "wall_coeffs_025 = np.percentile(trace, 2.5, axis=0)\n",
    "wall_coeffs_025 = decision_variables_to_dict(grouping, wall_coeffs_025)\n",
    "\n",
    "# compute 97.5th percentile parameters\n",
    "wall_coeffs_975 = np.percentile(trace, 97.5, axis=0)\n",
    "wall_coeffs_975 = decision_variables_to_dict(grouping, wall_coeffs_975)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddce430-2dfb-429c-81e1-f176831e9995",
   "metadata": {},
   "source": [
    "Run EPANET simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ba71e-7630-423c-ab42-f5e3dca60b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "flow_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-flow.csv\")\n",
    "pressure_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-pressure.csv\")\n",
    "wq_df = pd.read_csv(TIMESERIES_DIR / f\"processed/{str(data_period).zfill(2)}-wq.csv\", low_memory=False)\n",
    "cl_df = wq_df[wq_df['data_type'] == 'chlorine']\n",
    "\n",
    "# function to build models and run simulations\n",
    "def run_simulation(flow_df, pressure_df, cl_df, wall_coeffs, sim_type='chlorine', demand_resolution='wwmd', grouping=None):\n",
    "    wn = build_model(flow_df, pressure_df, cl_df, sim_type=sim_type, demand_resolution=demand_resolution, grouping=grouping, wall_coeffs=wall_coeffs)\n",
    "    sim_results = epanet_simulator(wn, sim_type, cl_df)\n",
    "    return sim_results.chlorine\n",
    "\n",
    "# run simulations for different wall coefficients\n",
    "cl_sim_mean = run_simulation(flow_df, pressure_df, cl_df, wall_coeffs=wall_coeffs_mean, grouping=grouping)\n",
    "cl_sim_025 = run_simulation(flow_df, pressure_df, cl_df, wall_coeffs=wall_coeffs_025, grouping=grouping)\n",
    "cl_sim_975 = run_simulation(flow_df, pressure_df, cl_df, wall_coeffs=wall_coeffs_975, grouping=grouping)\n",
    "\n",
    "# function to map simulated data to sensor nodes\n",
    "def map_to_sensors(sim_data, sensor_data):\n",
    "    name_mapping = sensor_data.set_index('model_id')['bwfl_id'].to_dict()\n",
    "    sim_data_sensor = sim_data[sensor_data['model_id'].unique()]\n",
    "    return sim_data_sensor.rename(columns=name_mapping)\n",
    "\n",
    "# obtain simulated data at sensor nodes\n",
    "sensor_data = sensor_model_id('wq')\n",
    "cl_sim_mean_sensor = map_to_sensors(cl_sim_mean, sensor_data)\n",
    "cl_sim_025_sensor = map_to_sensors(cl_sim_025, sensor_data)\n",
    "cl_sim_975_sensor = map_to_sensors(cl_sim_975, sensor_data)\n",
    "\n",
    "# filter sensors in observed data\n",
    "cl_df_bwfl_ids = cl_df['bwfl_id'].unique()\n",
    "bwfl_ids = [\n",
    "    sensor for sensor in sensor_data['bwfl_id'].unique() \n",
    "    if sensor in cl_df_bwfl_ids and sensor not in ['BW1', 'BW4']\n",
    "]\n",
    "\n",
    "datetime = cl_df['datetime'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c90de-26c2-45bc-a98c-409e4258d30e",
   "metadata": {},
   "source": [
    "Plot time series comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd68cd-2ba0-412b-9764-8465e53f5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bwfl_ids = [bwfl_id for bwfl_id in wq_df['bwfl_id'].unique() if bwfl_id not in ['BW1', 'BW4']]\n",
    "bwfl_ids = ['BW1', 'BW2', 'BW3', 'BW4', 'BW5', 'BW6', 'BW7', 'BW9', 'BW12']\n",
    "subplot_titles = [f\"{bwfl_id}\" for bwfl_id in bwfl_ids]\n",
    "fig = make_subplots(rows=len(bwfl_ids), cols=1, subplot_titles=subplot_titles)\n",
    "\n",
    "y_max = 1.0\n",
    "\n",
    "for idx, bwfl_id in enumerate(bwfl_ids): \n",
    "    data = wq_df[(wq_df['bwfl_id'] == bwfl_id) & (wq_df['data_type'] == 'chlorine')]\n",
    "    sim_mean = cl_sim_mean_sensor[bwfl_id].values\n",
    "    sim_025 = cl_sim_025_sensor[bwfl_id].values\n",
    "    sim_975 = cl_sim_975_sensor[bwfl_id].values\n",
    "    show_legend = (idx == 0)\n",
    "\n",
    "    # sensor data\n",
    "    if wq_sensors_used == 'kiosk only':\n",
    "        if bwfl_id in ['BW3', 'BW6', 'BW7']:\n",
    "            dash = 'dot'\n",
    "        else:\n",
    "            dash = 'solid'\n",
    "    elif bwfl_id in ['BW7']:\n",
    "        dash = 'dot'\n",
    "    else:\n",
    "        dash = 'solid'\n",
    "            \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data['datetime'][96:],\n",
    "            y=data['mean'][96:],\n",
    "            mode='lines',\n",
    "            name='actual',\n",
    "            line=dict(color='black', dash=dash),\n",
    "            showlegend=show_legend\n",
    "        ),\n",
    "        row=idx + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # simulated data\n",
    "    if bwfl_id != 'BW7':\n",
    "        color = color=default_colors[1]\n",
    "    else:\n",
    "        color = color=default_colors[5]\n",
    "        \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data['datetime'][96:],\n",
    "            y=sim_mean[96:],\n",
    "            mode='lines',\n",
    "            name='model',\n",
    "            line=dict(color=color),\n",
    "            showlegend=show_legend\n",
    "        ),\n",
    "        row=idx + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # add confidence interval\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data['datetime'][96:].tolist() + data['datetime'][96:].tolist()[::-1],\n",
    "            y=sim_975[96:].tolist() + sim_025[96:].tolist()[::-1],\n",
    "            fill='toself',\n",
    "            fillcolor=color,\n",
    "            opacity=0.2,\n",
    "            line=dict(color='rgba(0,0,0,0)'),\n",
    "            name='95% CI',\n",
    "            showlegend=show_legend\n",
    "        ),\n",
    "        row=idx + 1, col=1\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Chlorine [mg/L]\", rangemode=\"tozero\", range=[0, y_max], row=idx + 1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=450 * len(bwfl_ids),\n",
    "    template='simple_white',\n",
    "    showlegend=False,\n",
    "    font=dict(size=24),\n",
    "    title_font=dict(size=24),\n",
    "    width=800\n",
    ")\n",
    "for i in fig['layout']['annotations']:\n",
    "   i['font'] = dict(size=28)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4ddc6-b99a-42a9-bfa5-bf2449ee3358",
   "metadata": {},
   "source": [
    "Parity plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65788312-fa4b-4197-8b89-c6eb3f791ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=len(bwfl_ids), cols=1, subplot_titles=bwfl_ids)\n",
    "\n",
    "for idx, bwfl_id in enumerate(bwfl_ids):\n",
    "    row = idx + 1\n",
    "    col = 1\n",
    "    \n",
    "    # get data for this location\n",
    "    data = wq_df[(wq_df['bwfl_id'] == bwfl_id) & (wq_df['data_type'] == 'chlorine')]\n",
    "    measured = data['mean'][96:].values\n",
    "    simulated = cl_sim_mean_sensor[bwfl_id].values[96:]\n",
    "    \n",
    "    # calculate max value for axis limits\n",
    "    max_val = max(max(measured), max(simulated), 1.0)\n",
    "    \n",
    "    # add 1:1 line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, max_val],\n",
    "            y=[0, max_val],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', dash='solid'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # add error bounds\n",
    "    for offset in [-0.1, 0.1]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, max_val],\n",
    "                y=[offset, max_val + offset],\n",
    "                mode='lines',\n",
    "                line=dict(color='black', dash='dot'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # add scatter points\n",
    "    if bwfl_id != 'BW7':\n",
    "        color = color=default_colors[1]\n",
    "    else:\n",
    "        color = color=default_colors[5]\n",
    "        \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=measured,\n",
    "            y=simulated,\n",
    "            mode='markers',\n",
    "            marker=dict(color=color, size=10, opacity=0.6),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # update axes\n",
    "    fig.update_xaxes(title_text=\"Measured [mg/L]\", range=[0, max_val], row=row, col=col)\n",
    "    fig.update_yaxes(title_text=\"Simulated [mg/L]\", range=[0, max_val], row=row, col=col)\n",
    "\n",
    "# update layout\n",
    "fig.update_layout(\n",
    "    height=450 * len(bwfl_ids),\n",
    "    width=550,\n",
    "    template='simple_white',\n",
    "    showlegend=False,\n",
    "    font=dict(size=24)\n",
    ")\n",
    "\n",
    "# update subplot title font sizes\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=28)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907b8cf-0b2b-4d3f-8832-f349c38eb947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
